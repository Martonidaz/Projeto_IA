{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Data Understanding e Data Preparation\n",
    "# usada para carregar, organizar e manipular o conjunto de dados em um formato\n",
    "#  estruturado, como DataFrame\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o dataset\n",
    "penguins = pd.read_csv(\"penguins.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo as primeiras linhas do dataset\n",
    "penguins.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe o número de valores ausentes por coluna\n",
    "print(penguins.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para ajustar os dados a uma distribuição normal padrão, garantindo que todas as variáveis\n",
    "#  tenham média zero e desvio padrão igual a 1. Isso evita que variáveis com escalas maiores \n",
    "# dominem o modelo.\n",
    "\n",
    "# Classe uusada para normalizar ou padronizar os dados\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Seleção e descrição das variáveis numéricas para o modelo de clustering\n",
    "Xpenguins = penguins[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "\n",
    "# Tratamento de valores ausentes (substituição por média da coluna)\n",
    "Xpenguins = Xpenguins.fillna(Xpenguins.mean())\n",
    "\n",
    "# Normalização das variáveis numéricas\n",
    "scaler = StandardScaler()\n",
    "Xpenguins_scaled = scaler.fit_transform(Xpenguins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe o número de valores ausentes por coluna\n",
    "print(Xpenguins.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Modeling: Determinação do número ideal de clusters usando o método Elbow\n",
    "# Importa o algoritmo de agrupamento K-Means da biblioteca scikit-learn\n",
    "# erá utilizado para agrupar os dados em diferentes clusters, ou seja, \n",
    "# identificar padrões ou grupos no dataset.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "wcsse = []\n",
    "max_clusters = 10\n",
    "for i in range(1, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "    kmeans.fit(Xpenguins_scaled)\n",
    "    wcsse.append(kmeans.inertia_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Evaluation: Visualização e avaliação dos clusters formados\n",
    "\n",
    "# Biblioteca usada para criar gráficos e visualizações.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dados sem agrupamento\n",
    "\n",
    "# Definindo cores para cada cluster\n",
    "colors = ['purple'] \n",
    "labels = kmeans.labels_\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotando cada ponto com a cor do seu respectivo cluster\n",
    "for i in range(1):\n",
    "    plt.scatter(\n",
    "        Xpenguins_scaled[labels == i, 0], \n",
    "        Xpenguins_scaled[labels == i, 1], \n",
    "        s=50, \n",
    "        c=colors[i], \n",
    "        label=f'Sem agrupamento'\n",
    "    )\n",
    "\n",
    "\n",
    "plt.xlabel(\"Comprimento do culmen (bico)\")\n",
    "plt.ylabel(\"Profundidade do culmen (bico)\")\n",
    "plt.title(\"Dados\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando o gráfico do método do cotovelo (Elbow Method)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, max_clusters + 1), wcsse, marker='o')\n",
    "plt.title('Método do Cotovelo para Definir o Número de Clusters')\n",
    "plt.xlabel('Número de Clusters')\n",
    "plt.ylabel('WCSSE (Within-Cluster Sum of Squares)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o número de clusters com base no Elbow\n",
    "n_clusters = 3 \n",
    "\n",
    "# Parâmetros alternativos\n",
    "param_combinations = [ {\"init\": \"random\", \"n_clusters\": n_clusters, \"metric\": \"euclidean\"} ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicação dos modelos com diferentes parâmetros e avaliação\n",
    "results = []\n",
    "for params in param_combinations:\n",
    "    kmeans = KMeans(n_clusters=params[\"n_clusters\"], init=params[\"init\"], random_state=3)\n",
    "    kmeans.fit(Xpenguins_scaled)\n",
    "    \n",
    "    # Guardar os resultados\n",
    "    results.append({\n",
    "        \"inicialização\": params[\"init\"],\n",
    "        \"WCSSE\": kmeans.inertia_,\n",
    "        \"Centroides\": kmeans.cluster_centers_,\n",
    "        \"Labels\": kmeans.labels_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar os dados armazenados\n",
    "print(f\"Modelo com inicialização {params['init']} e distância {params['metric']}:\\n\")\n",
    "print(\"Centroides dos clusters:\\n\", kmeans.cluster_centers_, \"\\n\")\n",
    "print(\"WCSSE:\", kmeans.inertia_)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Evaluation: Visualização e avaliação dos clusters formados\n",
    "\n",
    "# Medida de distância Euclididana \n",
    "\n",
    "# Definindo cores para cada cluster\n",
    "colors = ['purple', 'turquoise', 'orange'] \n",
    "labels = kmeans.labels_\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotando cada ponto com a cor do seu respectivo cluster\n",
    "for i in range(n_clusters):  # Número de clusters = 3\n",
    "    plt.scatter(\n",
    "        Xpenguins_scaled[labels == i, 0], \n",
    "        Xpenguins_scaled[labels == i, 1], \n",
    "        s=50, \n",
    "        c=colors[i], \n",
    "        label=f'Cluster {i+1}'\n",
    "    )\n",
    "\n",
    "# Plotando os centroides em uma cor destacada\n",
    "plt.scatter(\n",
    "    kmeans.cluster_centers_[:, 0], \n",
    "    kmeans.cluster_centers_[:, 1], \n",
    "    s=100, \n",
    "    c='red', \n",
    "    marker='o', \n",
    "    edgecolor='black', \n",
    "    label=\"Centroides\"\n",
    ")\n",
    "\n",
    "# Legenda\n",
    "plt.xlabel(\"Comprimento do culmen (bico)\")\n",
    "plt.ylabel(\"Profundidade do culmen (bico)\")\n",
    "plt.title(\"Inicialização Random com Distância Euclidiana\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Criação de dataset de teste com uma única linha para verificação do modelo\n",
    "novo_pinguim = scaler.transform([[45.5, 17.4, 210, 4500]])\n",
    "cluster_predito = kmeans.predict(novo_pinguim)\n",
    "print(f\"O novo pinguim pertence ao cluster: {cluster_predito[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros alternativos\n",
    "param_combinations = [ {\"init\": \"k-means++\", \"n_clusters\": n_clusters, \"metric\": \"euclidean\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Evaluation: Visualização e avaliação dos clusters formados\n",
    "\n",
    "# Medida de distância Euclididana \n",
    "\n",
    "# Definindo cores para cada cluster\n",
    "colors = ['purple', 'turquoise', 'orange'] \n",
    "labels = kmeans.labels_\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotando cada ponto com a cor do seu respectivo cluster\n",
    "for i in range(n_clusters):  # Número de clusters = 3\n",
    "    plt.scatter(\n",
    "        Xpenguins_scaled[labels == i, 0], \n",
    "        Xpenguins_scaled[labels == i, 1], \n",
    "        s=50, \n",
    "        c=colors[i], \n",
    "        label=f'Cluster {i+1}'\n",
    "    )\n",
    "\n",
    "# Plotando os centroides em uma cor destacada\n",
    "plt.scatter(\n",
    "    kmeans.cluster_centers_[:, 0], \n",
    "    kmeans.cluster_centers_[:, 1], \n",
    "    s=100, \n",
    "    c='red', \n",
    "    marker='o', \n",
    "    edgecolor='black', \n",
    "    label=\"Centroides\"\n",
    ")\n",
    "\n",
    "# Legenda\n",
    "plt.xlabel(\"Comprimento do culmen (bico)\")\n",
    "plt.ylabel(\"Profundidade do culmen (bico)\")\n",
    "plt.title(\"Inicialização KMeans com Distância Euclidiana\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Criação de dataset de teste com uma única linha para verificação do modelo\n",
    "novo_pinguim = scaler.transform([[45.5, 17.4, 210, 4500]])\n",
    "cluster_predito = kmeans.predict(novo_pinguim)\n",
    "print(f\"O novo pinguim pertence ao cluster: {cluster_predito[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioteca SciPy para calcular distâncias entre dois conjuntos de pontos\n",
    "from scipy.spatial.distance import cdist\n",
    "# Fornece suporte para operações matemáticas e manipulação de arrays numéricos\n",
    "import numpy as np\n",
    "\n",
    "# Medida de distância Manhattan\n",
    "\n",
    "# Função para calcular o KMeans com distância de Manhattan\n",
    "def kmeans_manhattan(X, n_clusters, max_iters=300, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Inicializando os centroides aleatoriamente\n",
    "    centroids = X[np.random.choice(X.shape[0], n_clusters, replace=False)]\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        # Calculando a matriz de distâncias de Manhattan\n",
    "        distances = cdist(X, centroids, metric='cityblock')  # 'cityblock' é a distância de Manhattan\n",
    "        \n",
    "        # Atribuindo cada ponto ao cluster mais próximo\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Atualizando os centroides\n",
    "        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(n_clusters)])\n",
    "        \n",
    "        # Verificando se houve mudança nos centroides\n",
    "        if np.all(centroids == new_centroids):\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "    \n",
    "    return centroids, labels\n",
    "\n",
    "# Usando o kmeans_manhattan para calcular os clusters\n",
    "centroids, labels = kmeans_manhattan(Xpenguins_scaled, n_clusters)\n",
    "\n",
    "# Plotando os clusters com a distância de Manhattan\n",
    "colors = ['purple', 'turquoise', 'orange'] \n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotando os pontos com cores baseadas nos clusters\n",
    "for i in range(n_clusters):\n",
    "    plt.scatter(\n",
    "        Xpenguins_scaled[labels == i, 0], \n",
    "        Xpenguins_scaled[labels == i, 1], \n",
    "        s=50, \n",
    "        c=colors[i], \n",
    "        label=f'Cluster {i+1}'\n",
    "    )\n",
    "\n",
    "# Plotando os centroides (medoids)\n",
    "plt.scatter(\n",
    "    centroids[:, 0], \n",
    "    centroids[:, 1], \n",
    "    s=100, \n",
    "    c='red', \n",
    "    marker='o', \n",
    "    edgecolor='black', \n",
    "    label=\"Centroides\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Comprimento do culmen (bico)\")\n",
    "plt.ylabel(\"Profundidade do culmen (bico)\")\n",
    "plt.title(\"Incialização Random com Distância de Manhattan\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Predição para um novo pinguim (calculando a distância de Manhattan)\n",
    "novo_pinguim = scaler.transform([[45.5, 17.4, 210, 4500]])\n",
    "\n",
    "# Calculando as distâncias de Manhattan ao centroides\n",
    "distances = cdist(novo_pinguim, centroids, metric='cityblock')\n",
    "cluster_predito = np.argmin(distances)\n",
    "\n",
    "print(f\"O novo pinguim pertence ao cluster: {cluster_predito}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar os dados armazenados ao usar KMeans com distância de Manhattan\n",
    "print(f\"Modelo com inicialização aleatória e distância de Manhattan:\\n\")\n",
    "print(\"Centroides dos clusters:\\n\", centroids, \"\\n\")\n",
    "\n",
    "# Calculando a soma dos quadrados das distâncias intra-cluster (WCSSE)\n",
    "# Aqui adaptamos o cálculo para a distância de Manhattan\n",
    "wcsse = sum(\n",
    "    np.sum(cdist(Xpenguins_scaled[labels == i], [centroids[i]], metric='cityblock'))\n",
    "    for i in range(n_clusters)\n",
    ")\n",
    "print(\"WCSSE (Manhattan):\", wcsse)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Exibir a previsão do cluster de um novo pinguim\n",
    "novo_pinguim = scaler.transform([[45.5, 17.4, 210, 4500]])\n",
    "distances = cdist(novo_pinguim, centroids, metric='cityblock')\n",
    "cluster_predito = np.argmin(distances)\n",
    "\n",
    "print(f\"O novo pinguim pertence ao cluster: {cluster_predito}\")\n",
    "\n",
    "# Usando o kmeans_manhattan para calcular os clusters\n",
    "centroids, labels = kmeans_manhattan(Xpenguins_scaled, n_clusters)\n",
    "\n",
    "# Plotando os clusters com a distância de Manhattan\n",
    "colors = ['purple', 'turquoise', 'orange'] \n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotando os pontos com cores baseadas nos clusters\n",
    "for i in range(n_clusters):\n",
    "    plt.scatter(\n",
    "        Xpenguins_scaled[labels == i, 0], \n",
    "        Xpenguins_scaled[labels == i, 1], \n",
    "        s=50, \n",
    "        c=colors[i], \n",
    "        label=f'Cluster {i+1}'\n",
    "    )\n",
    "\n",
    "# Plotando os centroides (medoids)\n",
    "plt.scatter(\n",
    "    centroids[:, 0], \n",
    "    centroids[:, 1], \n",
    "    s=100, \n",
    "    c='red', \n",
    "    marker='o', \n",
    "    edgecolor='black', \n",
    "    label=\"Centroides\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Comprimento do culmen (bico)\")\n",
    "plt.ylabel(\"Profundidade do culmen (bico)\")\n",
    "plt.title(\"Incialização KMeans com Distância de Manhattan\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suposição: kmeans_manhattan está implementado ou disponível\n",
    "# Centroides e labels retornados pelo modelo KMeans ou KMedoids\n",
    "centroids, labels = kmeans_manhattan(Xpenguins_scaled, n_clusters)\n",
    "\n",
    "# Calculando o WCSSE com distância de Manhattan\n",
    "wcsse = sum(\n",
    "    np.sum(cdist(Xpenguins_scaled[labels == i], [centroids[i]], metric='cityblock'))\n",
    "    for i in range(n_clusters)\n",
    ")\n",
    "print(\"WCSSE (Manhattan):\", wcsse)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Previsão de cluster para novo ponto\n",
    "novo_pinguim = scaler.transform([[45.5, 17.4, 210, 4500]])  # Novo pinguim escalado\n",
    "distances = cdist(novo_pinguim, centroids, metric='cityblock')\n",
    "cluster_predito = np.argmin(distances)\n",
    "print(f\"O novo pinguim pertence ao cluster: {cluster_predito}\")\n",
    "\n",
    "# Plotando os clusters\n",
    "colors = ['purple', 'turquoise', 'orange']\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    plt.scatter(\n",
    "        Xpenguins_scaled[labels == i][:, 0],  # Correção nos índices\n",
    "        Xpenguins_scaled[labels == i][:, 1],\n",
    "        s=50,\n",
    "        c=colors[i],\n",
    "        label=f'Cluster {i+1}'\n",
    "    )\n",
    "\n",
    "plt.scatter(\n",
    "    centroids[:, 0],\n",
    "    centroids[:, 1],\n",
    "    s=100,\n",
    "    c='red',\n",
    "    marker='o',\n",
    "    edgecolor='black',\n",
    "    label=\"Centroides\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Comprimento do culmen (bico)\")\n",
    "plt.ylabel(\"Profundidade do culmen (bico)\")\n",
    "plt.title(\"KMeans com Distância de Manhattan\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando a biblioteca necessária\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Usando o KMeans com inicialização KMeans++ para calcular os clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=300, random_state=42)\n",
    "\n",
    "# Ajustando o modelo aos dados escalonados\n",
    "kmeans.fit(Xpenguins_scaled)\n",
    "\n",
    "# Recuperando os centroides e os rótulos dos clusters após o ajuste do modelo\n",
    "centroids = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Visualizando os clusters com a distância de Manhattan\n",
    "colors = ['purple', 'turquoise', 'orange']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotando os pontos com cores baseadas nos clusters\n",
    "for i in range(n_clusters):\n",
    "    plt.scatter(\n",
    "        Xpenguins_scaled[labels == i, 0], \n",
    "        Xpenguins_scaled[labels == i, 1], \n",
    "        s=50, \n",
    "        c=colors[i], \n",
    "        label=f'Cluster {i+1}'\n",
    "    )\n",
    "\n",
    "# Plotando os centroides (medoids)\n",
    "plt.scatter(\n",
    "    centroids[:, 0], \n",
    "    centroids[:, 1], \n",
    "    s=100, \n",
    "    c='red', \n",
    "    marker='o', \n",
    "    edgecolor='black', \n",
    "    label=\"Centroides\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Comprimento do culmen (bico)\")\n",
    "plt.ylabel(\"Profundidade do culmen (bico)\")\n",
    "plt.title(\"Distância de Manhattan e Inicialização KMeans++\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Exibir os dados armazenados\n",
    "print(f\"Modelo com inicialização KMeans++ e distância de Manhattan:\\n\")\n",
    "print(\"Centroides dos clusters:\\n\", centroids, \"\\n\")\n",
    "\n",
    "# Calculando a soma dos quadrados das distâncias intra-cluster (WCSSE) com Manhattan\n",
    "wcsse = sum(\n",
    "    np.sum(cdist(Xpenguins_scaled[labels == i], [centroids[i]], metric='cityblock'))\n",
    "    for i in range(n_clusters)\n",
    ")\n",
    "print(\"WCSSE (Manhattan):\", wcsse)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Predição para um novo pinguim (calculando a distância de Manhattan)\n",
    "novo_pinguim = scaler.transform([[45.5, 17.4, 210, 4500]])\n",
    "\n",
    "# Calculando as distâncias de Manhattan aos centroides\n",
    "distances = cdist(novo_pinguim, centroids, metric='cityblock')\n",
    "\n",
    "# Predição do cluster mais próximo\n",
    "cluster_predito = np.argmin(distances)\n",
    "\n",
    "print(f\"O novo pinguim pertence ao cluster: {cluster_predito}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar o número de pontos em cada cluster\n",
    "for i in range(n_clusters):\n",
    "    count = np.sum(labels == i)\n",
    "    print(f\"Cluster {i}: {count} pontos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Função para executar o KMeans com distância Euclidiana ou Manhattan\n",
    "def kmeans_with_distance(X, n_clusters, distance_metric, init_method='random'):\n",
    "    # Inicializando o KMeans com a distância especificada e o método de inicialização\n",
    "    if init_method == 'random':\n",
    "        kmeans = KMeans(n_clusters=n_clusters, init='random', max_iter=300, random_state=42)\n",
    "    elif init_method == 'k-means++':\n",
    "        kmeans = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=300, random_state=42)\n",
    "    \n",
    "    # Ajustando o modelo aos dados\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    # Caso a distância Manhattan seja utilizada, calculemos as distâncias de Manhattan\n",
    "    if distance_metric == 'manhattan':\n",
    "        labels = kmeans.labels_\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        wcsse = sum(\n",
    "            np.sum(cdist(X[labels == i], [centroids[i]], metric='cityblock')) \n",
    "            for i in range(n_clusters)\n",
    "        )\n",
    "        return kmeans, wcsse, labels, centroids\n",
    "    else:  # Distância Euclidiana\n",
    "        return kmeans, None, kmeans.labels_, kmeans.cluster_centers_\n",
    "\n",
    "# Função para aplicar o Percentage Split\n",
    "def percentage_split(X, n_clusters, distance_metric, init_method):\n",
    "    # Dividindo os dados em treinamento e teste (80% treino, 20% teste)\n",
    "    X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)[0], train_test_split(X, test_size=0.2, random_state=42)[1]\n",
    "    \n",
    "    # Treinando o modelo com os dados de treinamento\n",
    "    model, wcsse, labels, centroids = kmeans_with_distance(X_train, n_clusters, distance_metric, init_method)\n",
    "    \n",
    "    # Avaliando o desempenho nos dados de teste\n",
    "    test_labels = model.predict(X_test)\n",
    "    \n",
    "    # Silhouette score como métrica de avaliação\n",
    "    silhouette = silhouette_score(X_test, test_labels, metric='euclidean' if distance_metric == 'euclidean' else 'cityblock')\n",
    "    \n",
    "    print(f\"Modelo ({init_method}, {distance_metric}) - Silhouette Score:\", silhouette)\n",
    "    if wcsse:\n",
    "        print(f\"WCSSE (Manhattan) para o modelo ({init_method}, {distance_metric}):\", wcsse)\n",
    "\n",
    "# Função para aplicar a Cross-Validation\n",
    "def cross_validation(X, n_clusters, distance_metric, init_method):\n",
    "    # Avaliação utilizando Cross-Validation (5 dobras)\n",
    "    scores = cross_val_score(KMeans(n_clusters=n_clusters, init=init_method, max_iter=300, random_state=42),\n",
    "                             X, cv=5, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    print(f\"Modelo ({init_method}, {distance_metric}) - Cross-Validation MSE:\", -scores.mean())\n",
    "\n",
    "# Definir variáveis de entrada, como X (dados)\n",
    "# Exemplo: X = Xpenguins_scaled\n",
    "\n",
    "# Definindo o número de clusters\n",
    "n_clusters = 3  # Defina conforme o seu caso\n",
    "\n",
    "# Aplicando Percentage Split e Cross-Validation para todos os casos\n",
    "\n",
    "# Caso 1: Random e Euclidean\n",
    "print(\"\\nPercentage Split: Random e Euclidean\")\n",
    "percentage_split(Xpenguins_scaled, n_clusters=n_clusters, distance_metric='euclidean', init_method='random')\n",
    "print(\"\\nCross Validation: Random e Euclidean\")\n",
    "cross_validation(Xpenguins_scaled, n_clusters=n_clusters, distance_metric='euclidean', init_method='random')\n",
    "\n",
    "# Caso 2: KMeans++ e Euclidean\n",
    "print(\"\\nPercentage Split: KMeans++ e Euclidean\")\n",
    "percentage_split(Xpenguins_scaled, n_clusters=n_clusters, distance_metric='euclidean', init_method='k-means++')\n",
    "print(\"\\nCross Validation: KMeans++ e Euclidean\")\n",
    "cross_validation(Xpenguins_scaled, n_clusters=n_clusters, distance_metric='euclidean', init_method='k-means++')\n",
    "\n",
    "# Caso 3: Random e Manhattan\n",
    "print(\"\\nPercentage Split: Random e Manhattan\")\n",
    "percentage_split(Xpenguins_scaled, n_clusters=n_clusters, distance_metric='manhattan', init_method='random')\n",
    "print(\"\\nCross Validation: Random e Manhattan\")\n",
    "cross_validation(Xpenguins_scaled, n_clusters=n_clusters, distance_metric='manhattan', init_method='random')\n",
    "\n",
    "# Caso 4: KMeans++ e Manhattan\n",
    "print(\"\\nPercentage Split: KMeans++ e Manhattan\")\n",
    "percentage_split(Xpenguins_scaled, n_clusters=n_clusters, distance_metric='manhattan', init_method='k-means++')\n",
    "print(\"\\nCross Validation: KMeans++ e Manhattan\")\n",
    "cross_validation(Xpenguins_scaled, n_clusters=n_clusters, distance_metric='manhattan', init_method='k-means++')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Função de Cross-Validation usando Silhouette Score\n",
    "def cross_validation(X, n_clusters, distance_metric, init_method):\n",
    "    # Usando KFold para dividir os dados\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        \n",
    "        # Treinando o modelo KMeans com o método de inicialização e a métrica de distância\n",
    "        model, _, labels, _ = kmeans_with_distance(X_train, n_clusters, distance_metric, init_method)\n",
    "        \n",
    "        # Calculando o Silhouette Score para os dados de teste\n",
    "        test_labels = model.predict(X_test)\n",
    "        silhouette = silhouette_score(X_test, test_labels, metric='euclidean' if distance_metric == 'euclidean' else 'cityblock')\n",
    "        silhouette_scores.append(silhouette)\n",
    "    \n",
    "    print(f\"Modelo ({init_method}, {distance_metric}) - Cross-Validation Silhouette Score: {np.mean(silhouette_scores)}\")\n",
    "\n",
    "# Aplicando a Cross-Validation para os diferentes casos:\n",
    "print(\"\\nCross Validation: Random e Euclidean\")\n",
    "cross_validation(Xpenguins_scaled, n_clusters=n_clusters, distance_metric='euclidean', init_method='random')\n",
    "\n",
    "print(\"\\nCross Validation: KMeans++ e Euclidean\")\n",
    "cross_validation(Xpenguins_scaled, n_clusters=n_clusters, distance_metric='euclidean', init_method='k-means++')\n",
    "\n",
    "print(\"\\nCross Validation: Random e Manhattan\")\n",
    "cross_validation(Xpenguins_scaled, n_clusters=n_clusters, distance_metric='manhattan', init_method='random')\n",
    "\n",
    "print(\"\\nCross Validation: KMeans++ e Manhattan\")\n",
    "cross_validation(Xpenguins_scaled, n_clusters=n_clusters, distance_metric='manhattan', init_method='k-means++')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Função para calcular WCSSE\n",
    "def calculate_wcsse(X, labels, centroids):\n",
    "    wcsse = 0\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        cluster_points = X[labels == i]\n",
    "        wcsse += np.sum((cluster_points - centroid) ** 2)\n",
    "    return wcsse\n",
    "\n",
    "# Função de validação cruzada (KFold) usando WCSSE\n",
    "def cross_validation_wcsse(X, n_clusters, distance_metric, init_method):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    total_wcsse = 0\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "\n",
    "        # Escolhendo o algoritmo de acordo com a métrica\n",
    "        algorithm = 'elkan' if distance_metric == 'euclidean' else 'lloyd'\n",
    "\n",
    "        # Criando o modelo KMeans\n",
    "        kmeans = KMeans(\n",
    "            n_clusters=n_clusters,\n",
    "            init=init_method,\n",
    "            algorithm=algorithm,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Ajustando o modelo no conjunto de treinamento\n",
    "        kmeans.fit(X_train)\n",
    "\n",
    "        # Prevendo os rótulos no conjunto de teste\n",
    "        test_labels = kmeans.predict(X_test)\n",
    "\n",
    "        # Calculando WCSSE para o conjunto de teste\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        wcsse = calculate_wcsse(X_test, test_labels, centroids)\n",
    "        total_wcsse += wcsse\n",
    "\n",
    "    print(f\"Modelo ({init_method}, {distance_metric}) - WCSSE Total (Cross Validation): {total_wcsse}\")\n",
    "\n",
    "# Função de divisão por porcentagem (Percentage Split) usando WCSSE\n",
    "def percentage_split_wcsse(X, n_clusters, distance_metric, init_method, test_size=0.3):\n",
    "    # Dividindo os dados em treinamento e teste\n",
    "    X_train, X_test = train_test_split(X, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Escolhendo o algoritmo de acordo com a métrica\n",
    "    algorithm = 'elkan' if distance_metric == 'euclidean' else 'lloyd'\n",
    "\n",
    "    # Criando o modelo KMeans\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        init=init_method,\n",
    "        algorithm=algorithm,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Ajustando o modelo no conjunto de treinamento\n",
    "    kmeans.fit(X_train)\n",
    "\n",
    "    # Prevendo os rótulos no conjunto de teste\n",
    "    test_labels = kmeans.predict(X_test)\n",
    "\n",
    "    # Calculando WCSSE para o conjunto de teste\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    wcsse = calculate_wcsse(X_test, test_labels, centroids)\n",
    "\n",
    "    print(f\"Modelo ({init_method}, {distance_metric}) - WCSSE Total (Percentage Split): {wcsse}\")\n",
    "\n",
    "# Exemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Carregando o dataset Iris\n",
    "    data = load_iris()\n",
    "    X = data.data\n",
    "\n",
    "    # Normalizando os dados\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Parâmetros de clustering\n",
    "    n_clusters = 3\n",
    "\n",
    "    # Validação cruzada\n",
    "    print(\"\\nCross Validation: KMeans++ e Euclidean\")\n",
    "    cross_validation_wcsse(X_scaled, n_clusters=n_clusters, distance_metric='euclidean', init_method='k-means++')\n",
    "\n",
    "    print(\"\\nCross Validation: Random e Manhattan\")\n",
    "    cross_validation_wcsse(X_scaled, n_clusters=n_clusters, distance_metric='manhattan', init_method='random')\n",
    "\n",
    "    # Divisão por porcentagem\n",
    "    print(\"\\nPercentage Split: KMeans++ e Euclidean\")\n",
    "    percentage_split_wcsse(X_scaled, n_clusters=n_clusters, distance_metric='euclidean', init_method='k-means++')\n",
    "\n",
    "    print(\"\\nPercentage Split: Random e Manhattan\")\n",
    "    percentage_split_wcsse(X_scaled, n_clusters=n_clusters, distance_metric='manhattan', init_method='random')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Função para calcular as médias das variáveis por cluster\n",
    "def cluster_summary(X, labels):\n",
    "    df = pd.DataFrame(X, columns=['culmen_length_mm', 'culmen_depth_mm ', 'body_mass_g ', 'flipper_length_mm '])\n",
    "    df['Cluster'] = labels\n",
    "    cluster_means = df.groupby('Cluster').mean()\n",
    "    print(\"\\nMédias das variáveis por cluster:\")\n",
    "    print(cluster_means)\n",
    "\n",
    "# Após aplicar o KMeans, calcule as médias para cada cluster\n",
    "model = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
    "model.fit(Xpenguins_scaled)\n",
    "labels = model.labels_\n",
    "\n",
    "cluster_summary(Xpenguins_scaled, labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
