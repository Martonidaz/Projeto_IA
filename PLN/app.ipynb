{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 1: Carregar o dataset\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"Historico_de_materias.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para exibir somente os tipos de assunto\n",
    "valores_unicos = dados['assunto'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores únicos na coluna 'assunto':\n",
      "['economia' 'esportes' 'politica' 'famosos' 'tecnologia']\n"
     ]
    }
   ],
   "source": [
    "print(\"Valores únicos na coluna 'assunto':\")\n",
    "print(valores_unicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 2: Criar um novo dataframe somente para o assunto 'esporte'\n",
    "dados_esportes = dados[dados['assunto'] == 'esportes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_esportes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 3: Carregar spaCy\n",
    "# Biblioteca de processamento de linguagem natural para tarefas como tokenização, \n",
    "# lematização entre outras.\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo pré-treinado otimizado para a língua portuguesa\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 4: Configurar palavras de parada\n",
    "\n",
    "# Essa biblioteca oferece ferramentas para análise de texto, como stemmers (radicalizadores),\n",
    "#  stopwords, tokenizadores, e entre outras funções\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o conjunto de palavras de parada\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar stopwords\n",
    "# lista padrão da biblioteca pré-definida de palavras irrelevantes\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega as stopwords em português do NLTK e as converte em um conjunto Python\n",
    "api_stop_words = set(stopwords.words('portuguese'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incluir vogais isoladas não parece contribuir para o significado.\n",
    "minhas_stop_words = {'a', 'e', 'i', 'o', 'u', 'né', 'aí', 'tá', 'então'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinação das palavras de parada padrão do NLTK com \"minhas_stop_words\" \n",
    "stop_words = api_stop_words | minhas_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 5: Função de tratamento de texto\n",
    "\n",
    "# Biblioteca padrão do Python para expressões regulares\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expressões regulares permitem manipular e encontrar padrões em strings, \n",
    "# como remover caracteres indesejados ou verificar formatos específicos (ex.: datas, e-mails).\n",
    "# Para realizar a limpeza do texto removendo caracteres que não sejam letras ou espaços.\n",
    "\n",
    "# função usada para pré-processar textos antes de realizar análises mais avançadas.\n",
    "def tratamento_pln(texto):\n",
    "    # Converte todo o texto para letras minúsculas\n",
    "    texto = texto.lower()\n",
    "    # Substitui todos os caracteres indesejados por uma string vazia\n",
    "    texto = re.sub(r'[^a-zA-Záéíóú\\s]', '', texto)\n",
    "    # Lematiza as palavras de acordo com o pipeline da spacy e divide o texto em tokens\n",
    "    doc = nlp(texto)\n",
    "    # Cria uma lista de palavras tratadas que não são pontuações nem stopwords\n",
    "    clean_tokens = [token.lemma_ for token in doc if token.text not in stop_words and not token.is_punct]\n",
    "    # Combina a lista de palavras limpas (clean_tokens) em uma única string\n",
    "    clean_text = ' '.join(clean_tokens)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do conjunto stop_words: 213 \n",
      "Stop_words ordenadas: \n",
      " ['a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'até', 'aí', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'então', 'era', 'eram', 'essa', 'essas', 'esse', 'esses', 'esta', 'estamos', 'estar', 'estas', 'estava', 'estavam', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéramos', 'estivéssemos', 'estou', 'está', 'estávamos', 'estão', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'forem', 'formos', 'fosse', 'fossem', 'fui', 'fôramos', 'fôssemos', 'haja', 'hajam', 'hajamos', 'havemos', 'haver', 'hei', 'houve', 'houvemos', 'houver', 'houvera', 'houveram', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houvermos', 'houverá', 'houverão', 'houveríamos', 'houvesse', 'houvessem', 'houvéramos', 'houvéssemos', 'há', 'hão', 'i', 'isso', 'isto', 'já', 'lhe', 'lhes', 'mais', 'mas', 'me', 'mesmo', 'meu', 'meus', 'minha', 'minhas', 'muito', 'na', 'nas', 'nem', 'no', 'nos', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'não', 'né', 'nós', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 'qual', 'quando', 'que', 'quem', 'se', 'seja', 'sejam', 'sejamos', 'sem', 'ser', 'serei', 'seremos', 'seria', 'seriam', 'será', 'serão', 'seríamos', 'seu', 'seus', 'somos', 'sou', 'sua', 'suas', 'são', 'só', 'também', 'te', 'tem', 'temos', 'tenha', 'tenham', 'tenhamos', 'tenho', 'terei', 'teremos', 'teria', 'teriam', 'terá', 'terão', 'teríamos', 'teu', 'teus', 'teve', 'tinha', 'tinham', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéramos', 'tivéssemos', 'tu', 'tua', 'tuas', 'tá', 'tém', 'tínhamos', 'u', 'um', 'uma', 'você', 'vocês', 'vos', 'à', 'às', 'é', 'éramos']\n"
     ]
    }
   ],
   "source": [
    "# Visualizando as Stop Words\n",
    "print(\"Tamanho do conjunto stop_words:\",len(stop_words),\"\\nStop_words ordenadas: \\n\",sorted(list(stop_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar o tratamento\n",
    "# Usada para dividir um texto grande em partes menores (chunks) de tamanho controlado.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_10696\\723068.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dados_esportes['conteudo_noticia'] = dados_esportes['conteudo_noticia'].fillna(\"\").astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Substitui valores ausentes por uma string vazia e converte todos os valores da coluna \n",
    "# \"conteudo_noticia\" para o tipo string\n",
    "dados_esportes['conteudo_noticia'] = dados_esportes['conteudo_noticia'].fillna(\"\").astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_10696\\2699809724.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dados_esportes['conteudo_tratado'] = dados_esportes['conteudo_noticia'].apply(tratamento_pln)\n"
     ]
    }
   ],
   "source": [
    "# Aplica a função tratamento_pln para cada valor na coluna \"conteudo_noticia\"\n",
    "dados_esportes['conteudo_tratado'] = dados_esportes['conteudo_noticia'].apply(tratamento_pln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 6: Dividir o texto em partes (chunks)\n",
    "# Para controlar o tamanho \n",
    "# Cria um objeto text_splitter da classe RecursiveCharacterTextSplitter com os seguintes parâmetros:\n",
    "# chunk_size=200 tamanho máximo dos pedaços de texto (chunks) como 200 caracteres.\n",
    "# chunk_overlap=50: os pedaços de texto terão uma sobreposição de 50 caracteres entre eles.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista de chunks de todos os textos na coluna \"conteudo_tratado\"\n",
    "chunks = [\n",
    "    # Itera por cada notícia (linha) na coluna conteudo_tratado\n",
    "    chunk for noticia in dados_esportes['conteudo_tratado'] \n",
    "    # Para cada notícia, o split_text divide o texto em partes menores (chunks), \n",
    "    # de acordo com os parâmetros definidos anteriormente (tamanho de 200 caracteres e\n",
    "    #  sobreposição de 50 caracteres)\n",
    "    for chunk in text_splitter.split_text(noticia)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_10696\\2142784576.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dados_esportes['data'] = pd.to_datetime(dados['data'])\n"
     ]
    }
   ],
   "source": [
    "# Adiciona uma coluna de ano ou mês baseado na data (supondo que exista a coluna 'data')\n",
    "dados_esportes['data'] = pd.to_datetime(dados['data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_10696\\1175460100.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dados_esportes['ano_mes'] = dados_esportes['data'].dt.to_period('M')\n"
     ]
    }
   ],
   "source": [
    "dados_esportes['ano_mes'] = dados_esportes['data'].dt.to_period('M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conta a frequência de palavras por período\n",
    "palavras = dados_esportes['conteudo_tratado'].str.split(expand=True).stack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencia = palavras.groupby([dados_esportes['ano_mes']]).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Frequência das 10 palavras mais comuns por período\n",
    "print(frequencia.head(10))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\OneDrive\\Documentos\\GitRobson\\Projeto_IA_Walmir\\PLN\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Daniel\\OneDrive\\Documentos\\GitRobson\\Projeto_IA_Walmir\\PLN\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Passo 7: Codificar os pedaços\n",
    "# Biblioteca popular para gerar representações vetoriais (embeddings) de sentenças ou\n",
    "# trechos de texto, converte textos em vetores numéricos que podem ser usados para tarefas\n",
    "# como medir similaridade, agrupamento ou busca semântica.\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um modelo pré-treinado para gerar embeddings de texto em português (e outras línguas) \n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codifica todos os pedaços de texto (chunks) gerados anteriormente da lista \"chunks\" em vetores\n",
    "embeddings = model.encode(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 8: Configurar ChromaDB\n",
    "# Biblioteca ara criar e gerenciar bancos de dados vetoriais, ou seja,\n",
    "# armazenar e consultar embeddings de texto\n",
    "# chromadb facilita a criação de coleções de dados vetoriais permitindo realizar operações,\n",
    "# como consulta por similaridade.\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte embeddings gerados pelo modelo SentenceTransformer para o formato adequado que o ChromaDB\n",
    "#  Assim, a integração entre o modelo de embeddings e o banco de dados vetorial\n",
    "#  se torna direta e simplificada.\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar o ChromaDB com a função de embeddings\n",
    "# conecta a biblioteca chromadb ao modelo SentenceTransformer, permitindo que o ChromaDB \n",
    "# converta textos em embeddings com base no modelo all-MiniLM-L6-v2.\n",
    "embedding_function = SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O client é a interface principal que você usa para interagir com o ChromaDB\n",
    "client = chromadb.Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.delete_collection(\"esportes_noticias\")\n",
    "# Cria uma coleção chamada \"esportes_noticias\" no ChromaDB\n",
    "collection = client.create_collection(\"esportes_noticias\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar documentos ao banco de dados vetorial\n",
    "# Adiciona cada pedaço de texto (chunk) ao banco de dados vetorial.\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Adiciona cada chunk à coleção \"esportes_noticias\" no ChromaDB\n",
    "    collection.add(documents=[chunk], metadatas=[{'id': i}], ids=[str(i)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para encontrar notícias semelhantes\n",
    "def encontrar_similares(texto):\n",
    "    # Gerar embeddings para a nova notícia\n",
    "    # texto_tratado será o texto limpo e tratado, pronto para ser transformado em um embedding.\n",
    "    texto_tratado = tratamento_pln(texto)\n",
    "    # Gera um embedding para o texto tratado utilizando o modelo de embeddings SentenceTransformer\n",
    "    embedding_novo = model.encode([texto_tratado])\n",
    "    \n",
    "    # Consultar no ChromaDB\n",
    "    # Realiza uma consulta no banco de dados vetorial do ChromaDB \n",
    "    resultados = collection.query(\n",
    "        # Passa o embedding da nova notícia como consulta para o banco de dados.\n",
    "        query_embeddings=embedding_novo,\n",
    "        # Número de resultados que deseja recuperar\n",
    "        n_results=5, \n",
    "    )\n",
    "    \n",
    "    # Contém os textos dos documentos mais semelhantes encontrados no ChromaDB\n",
    "    return resultados['documents'], resultados['metadatas']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspeção dos resultados\n",
    "print(\"Notícias semelhantes:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada à função encontrar_similares\n",
    "nova_noticia = \"O jogador marcou um gol decisivo na final do campeonato.\"\n",
    "encontrar_similares(nova_noticia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Função para encontrar notícias semelhantes e calcular a similaridade de cosseno\n",
    "def encontrar_similares(texto):\n",
    "    # Gerar embeddings para a nova notícia\n",
    "    texto_tratado = tratamento_pln(texto)\n",
    "    embedding_novo = model.encode([texto_tratado])\n",
    "    \n",
    "    # Consultar no ChromaDB\n",
    "    resultados = collection.query(\n",
    "        query_embeddings=embedding_novo,\n",
    "        n_results=5,  # Quantidade de resultados desejados\n",
    "    )\n",
    "    \n",
    "    # Calcular similaridade de cosseno entre o embedding da nova notícia e os encontrados\n",
    "    similaridades = cosine_similarity(embedding_novo, embeddings).flatten()\n",
    "    \n",
    "    # Retornar as notícias semelhantes, seus metadados e os graus de similaridade\n",
    "    documentos = resultados['documents']\n",
    "    metadados = resultados['metadatas']\n",
    "    ids = [int(metadata['id']) for metadata in metadados]\n",
    "    graus_similaridade = similaridades[ids]\n",
    "    \n",
    "    return documentos, metadados, graus_similaridade\n",
    "\n",
    "# Testar a função\n",
    "nova_noticia = \"O jogador marcou um gol decisivo na final do campeonato.\"\n",
    "noticias_similares, metadatas_similares, similaridades = encontrar_similares(nova_noticia)\n",
    "\n",
    "# Exibir os resultados\n",
    "print(\"Notícias semelhantes e graus de similaridade:\\n\")\n",
    "for noticia, metadata, similaridade in zip(noticias_similares[0], metadatas_similares[0], similaridades):\n",
    "    print(f\"Notícia: {noticia}\")\n",
    "    print(f\"Metadata: {metadata}\")\n",
    "    print(f\"Grau de Similaridade (Cosseno): {similaridade:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que você tenha uma tabela de frequências de palavras ao longo do tempo\n",
    "# Exemplo para a palavra \"gol\"\n",
    "frequencia_palavra = frequencia['gol']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencia_palavra.plot(kind='line', title='Tendência de uso da palavra \"gol\"', ylabel='Frequência', xlabel='Tempo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 8: Clusterização (agrupamento)\n",
    "# Definir número de clusters\n",
    "num_clusters = 5  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_esportes['cluster'] = kmeans.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 9: Geração de resumos\n",
    "def gerar_resumo(texto, num_sentencas=2):\n",
    "    doc = nlp(texto)\n",
    "    sentencas = [sent.text for sent in doc.sents]\n",
    "    return \" \".join(sentencas[:num_sentencas])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_esportes['resumo'] = dados_esportes['conteudo_noticia'].apply(lambda x: gerar_resumo(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar a frequência de clusters ao longo do tempo\n",
    "frequencia_clusters = dados_esportes.groupby(['ano_mes', 'cluster']).size().unstack(fill_value=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar tendências\n",
    "frequencia_clusters.plot(kind='line', figsize=(12, 6), marker='o')\n",
    "plt.title(\"Tendências de Tópicos ao Longo do Tempo\")\n",
    "plt.xlabel(\"Ano-Mês\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir amostra de resultados\n",
    "print(\"\\nExemplo de Resultados:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dados_esportes[['data_publicacao', 'cluster', 'resumo']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise adicional (opcional): Principais palavras-chave por cluster\n",
    "def obter_palavras_chave(cluster_id, top_n=5):\n",
    "    textos = dados_esports[dados_esportes['cluster'] == cluster_id]['conteudo_tratado']\n",
    "    palavras = \" \".join(textos).split()\n",
    "    mais_comuns = Counter(palavras).most_common(top_n)\n",
    "    return [palavra for palavra, _ in mais_comuns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(num_clusters):\n",
    "    palavras_chave = obter_palavras_chave(cluster)\n",
    "    print(f\"Cluster {cluster}: Palavras-chave - {', '.join(palavras_chave)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
